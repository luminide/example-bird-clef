# BirdCLEF 2022

### Introduction
This repository contains source code generated by [Luminide](https://luminide.com). It may be used to train, validate and tune deep learning models for audio classification. The following directory structure is assumed:
```
├── code (source code)
├── input (dataset)
└── output (working directory)
```

The dataset should have audio files inside a directory named `train_audio` and a CSV file named `train_metadata.csv`. An example is shown below:

```
input
├── train_metadata.csv
└── train_audio
    ├── 800113bb65efe69e.ogg
    ├── 8002cb321f8bfcdf.ogg
    ├── 80070f7fb5e2ccaa.ogg
```

The CSV file is expected to have labels under a column named `primary_label` as in the example below:

```
filename,primary_label
800113bb65efe69e.ogg,pacific_wren
8002cb321f8bfcdf.ogg,hermit_thrush pacific_wren
80070f7fb5e2ccaa.ogg,hermit_thrush
```
If an item has multiple labels, they should be separated by a space character as shown.

### To use this repo with Luminide
- Accept [competition rules](https://www.kaggle.com/competitions/birdclef-2022/rules).
- Attach a Compute Server that has a GPU (e.g. gcp-t4).
- Configure your [Kaggle API token](https://github.com/Kaggle/kaggle-api) on the `Import Data` tab.
- On the `Import Data` tab, choose Kaggle Competition Data and then enter `birdclef-2022`.
- Install required packages on the Compute Server using the Run Experiment menu (select `Custom > install.sh`).
- Train a model using the `Run Experiment` menu.

### Kaggle submission
- Upload the code to Kaggle as a dataset by using the Run Experiment menu (select `Custom > kaggle.sh`).
- To create a submission, copy [kaggle.ipynb](kaggle.ipynb) to a new Kaggle notebook.
- Add the notebook output of `https://www.kaggle.com/luminide/wheels1` as Data.
- Add your dataset at `https://www.kaggle.com/<kaggle_username>/kagglecode` as Data.
- Add the relevant competition dataset as Data.
- Save the notebook after turning off the `Internet` setting and turning on the GPU.
- Submit the results and wait for the notebook to finish.
- Check the [leaderboard](https://www.kaggle.com/competitions/birdclef-2022/leaderboard) to see your score!

### Additional features
- Use the `Experiment Tracking` menu to track experiments.
- To tune the hyperparameters, edit [sweep.yaml](sweep.yaml) as desired and launch a sweep from the `Run Experiment` tab. Tuned values will be copied back to a file called `config-tuned.yaml` along with visualizations in `sweep-results.html`.
- To use the tuned hyperparameter values, copy them over to `config.yaml` before training a model.
- For exploratory analysis, run [eda.ipynb](eda.ipynb).
- To monitor training progress, use the `Experiment Visualization` menu.
- After an experiment is complete, use the file browser on the IDE interface to access the results on the IDE Server.
- To generate a report on the most recent training session, run report.sh from the `Run Experiment` tab. Make sure `Track Experiment` is checked. The results will be copied back to a file called `report.html`.


### Acknowledgements
- The network architecture for the attention head is adapted from a [notebook by tattaka](https://www.kaggle.com/code/tattaka/birdclef2022-submission-baseline)

For more details on usage, see [Luminide documentation](https://luminide.readthedocs.io)
